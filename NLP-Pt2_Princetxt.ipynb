{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Little Prince\n",
      "written and illustrated by\n",
      "Antoi\n",
      "90305\n"
     ]
    }
   ],
   "source": [
    "# importing required nltk libraries\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "f = open('TheLittlePrince.txt',encoding='utf-8')\n",
    "text = f.read()\n",
    "print(text[:50])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Little Prince\\nwritten and illustrated by\\nAntoine de Saint Exupéry\\ntranslated from the French by Katherine Woods\\n\\nOnce when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest.', 'It was a picture of a boa constrictor in the act of swallowing an animal.', 'Here is a copy of the drawing.', 'In the book it said: \"Boa constrictors swallow their prey whole, without chewing it.', 'After that they are not able to move, and they sleep through the six months that they need for digestion.\"', 'I pondered deeply, then, over the adventures of the jungle.', 'And after some work with a colored pencil I succeeded in making my first drawing.', 'My Drawing Number One.', 'It looked something like this:\\n\\nDrawing Number One\\nI showed my masterpiece to the grown-ups, and asked them whether the drawing frightened them.', 'But they answered: \"Frighten?']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, as explained in the Labs\n",
    "# Separate the text into sentences first\n",
    "textsplit = nltk.sent_tokenize(text)\n",
    "print(textsplit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the word tokenizer to each sentence\n",
    "lowertext = [w.lower() for w in textsplit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'little', 'prince', 'written', 'and', 'illustrated', 'by', 'antoine', 'de', 'saint', 'exupéry', 'translated', 'from', 'the', 'french', 'by', 'katherine', 'woods', 'once', 'when', 'i', 'was', 'six', 'years', 'old', 'i', 'saw', 'a', 'magnificent', 'picture', 'in', 'a', 'book', ',', 'called', 'true', 'stories', 'from', 'nature', ',', 'about', 'the', 'primeval', 'forest', '.'], ['it', 'was', 'a', 'picture', 'of', 'a', 'boa', 'constrictor', 'in', 'the', 'act', 'of', 'swallowing', 'an', 'animal', '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1846"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokentext = [nltk.word_tokenize(sent) for sent in lowertext]\n",
    "print(tokentext[:2])\n",
    "#the output is a list of strings that contains the sentences\n",
    "type(tokentext)\n",
    "len(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('the', 'DT'), ('little', 'JJ'), ('prince', 'NN'), ('written', 'VBN'), ('and', 'CC'), ('illustrated', 'VBN'), ('by', 'IN'), ('antoine', 'NN'), ('de', 'IN'), ('saint', 'NN'), ('exupéry', 'NN'), ('translated', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('french', 'JJ'), ('by', 'IN'), ('katherine', 'JJ'), ('woods', 'NNS'), ('once', 'RB'), ('when', 'WRB'), ('i', 'NN'), ('was', 'VBD'), ('six', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('i', 'NN'), ('saw', 'VBD'), ('a', 'DT'), ('magnificent', 'JJ'), ('picture', 'NN'), ('in', 'IN'), ('a', 'DT'), ('book', 'NN'), (',', ','), ('called', 'VBN'), ('true', 'JJ'), ('stories', 'NNS'), ('from', 'IN'), ('nature', 'NN'), (',', ','), ('about', 'IN'), ('the', 'DT'), ('primeval', 'NN'), ('forest', 'NN'), ('.', '.')], [('it', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('picture', 'NN'), ('of', 'IN'), ('a', 'DT'), ('boa', 'NN'), ('constrictor', 'NN'), ('in', 'IN'), ('the', 'DT'), ('act', 'NN'), ('of', 'IN'), ('swallowing', 'VBG'), ('an', 'DT'), ('animal', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "## POS Tagging, to retrieve adjective (JJs) and adverb (RBs) tags\n",
    "\n",
    "# use the Stanford POS tagger to POS tag tokens of each sentence\n",
    "# this is the default tagger in nltk\n",
    "taggedtext = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adjective phrases:  ['not able ', 'not able ', 'very useful ', 'greatly pleased ', 'so i ', 'scarcely enough ', 'more isolated ', 'most extraordinary ', 'too overpowering ', 'so i ']\n",
      "Top adjective phrases by frequency: \n",
      "so i  6\n",
      "so much  6\n",
      "so small  5\n",
      "very much  5\n",
      "not able  4\n",
      "very small  4\n",
      "not important  3\n",
      "very useful  2\n",
      "once more  2\n",
      "too much  2\n",
      "so important  2\n",
      "very first  2\n",
      "too small  2\n",
      "so little  2\n",
      "so sad  2\n",
      "very busy  2\n",
      "very simple  2\n",
      "very beautiful  2\n",
      "much more difficult  2\n",
      "very strange  2\n",
      "very different  2\n",
      "so weak  2\n",
      "not worth  2\n",
      "too heavy  2\n",
      "greatly pleased  1\n",
      "scarcely enough  1\n",
      "more isolated  1\n",
      "most extraordinary  1\n",
      "too overpowering  1\n",
      "very dangerous  1\n",
      "very cumbersome  1\n",
      "too old  1\n",
      "very surprised  1\n",
      "not so small  1\n",
      "so many  1\n",
      "so good  1\n",
      "much greater  1\n",
      "no longer interested  1\n",
      "too tall  1\n",
      "too short  1\n",
      "now good  1\n",
      "now bad  1\n",
      "generally fair-to-middling  1\n",
      "not little  1\n",
      "as big  1\n",
      "so big  1\n",
      "strictly correct  1\n",
      "too many  1\n",
      "very tedious  1\n",
      "very easy  1\n",
      "Length of adjective phrase sentences:  183\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, chapter on Information Extraction--Chunking (https://www.nltk.org/book/ch07.html)\n",
    "\n",
    "# Using CHUNKING to parse sentences \n",
    "# to look for \"adjective phrases\", i.e. phrases (or chunks) that have adverbs and adjectives ('RB'+'JJ')\n",
    "# First step: writing a grammar that defines the POS in the chunk\n",
    "# we name this grammar \"ADJPH\" (\"ADJective PHrase\") using regexes \n",
    "\n",
    "import re\n",
    "grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\"\n",
    "# This regex reads as: \"find groups (\"< >\") of RBs (adverbs) together with groups of JJs (adjectives), with groups defineds as\n",
    "# RBs with any ending (the \".\" is a placeholder or wildcard for the \"R\" and the \"S\" at the end of RBR and RBS, \n",
    "# while \"?\" indicates \"optional character\" so RB can be found alone as well). Same regex operators apply to JJs.\n",
    "# Feel free to edit the regex in this grammar, or come up with your own.\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\n",
    "\n",
    "adjph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adj.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADJPH':\n",
    "                adjph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adjective_phrases = []\n",
    "for sent in adjph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adjective_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adjective phrases: ', adjective_phrases[:10])\n",
    "\n",
    "\n",
    "# Following our NLTK textbook, chapter 1 on Language Processing (https://www.nltk.org/book/ch01.html)\n",
    "\n",
    "## FREQUENCY DISTRIBUTIONS\n",
    "# Top 50 adjective phrases\n",
    "freq_adjph = nltk.FreqDist(adjective_phrases)\n",
    "\n",
    "print('Top adjective phrases by frequency: ')\n",
    "for word, freq in freq_adjph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adjective phrase sentences: ', len(adjph_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  freq\n",
      "adj                   \n",
      "not able             4\n",
      "very useful          2\n",
      "greatly pleased      1\n",
      "so i                 6\n",
      "scarcely enough      1\n",
      "...                ...\n",
      "so naïve             1\n",
      "too sat              1\n",
      "well content         1\n",
      "not such             1\n",
      "so now i             1\n",
      "\n",
      "[140 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#freq_adjph\n",
    "df_fadj = pd.DataFrame.from_dict(freq_adjph, orient='index')\n",
    "df_fadj.columns = ['freq']\n",
    "df_fadj.index.name = 'adj'\n",
    "print(df_fadj)\n",
    "\n",
    "#df = pd.DataFrame(freq_adjph, columns = ['word','freq'])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "freqfreq = df_fadj['freq'].value_counts()\n",
    "\n",
    "freqfreq.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adverb phrases:  ['so then ', \"n't much \", 'certainly very much ', 'now i ', 'very slowly ', 'not disobey ', 'little crossly ', 'so often ', 'so then ', 'already very sickly ']\n",
      "Top adverb phrases by frequency: \n",
      "so then  5\n",
      "far away  4\n",
      "not even  4\n",
      "not so  3\n",
      "very much  3\n",
      "very well  3\n",
      "just so  3\n",
      "once again  3\n",
      "very slowly  2\n",
      "much too  2\n",
      "no longer  2\n",
      "not yet  2\n",
      "certainly very  2\n",
      "not very  2\n",
      "very rarely  2\n",
      "too often  2\n",
      "just alike  2\n",
      "too long  2\n",
      "n't much  1\n",
      "certainly very much  1\n",
      "now i  1\n",
      "not disobey  1\n",
      "little crossly  1\n",
      "so often  1\n",
      "already very sickly  1\n",
      "very far away  1\n",
      "very far  1\n",
      "not really  1\n",
      "only once  1\n",
      "over again  1\n",
      "inoffensively upward  1\n",
      "as soon  1\n",
      "too late  1\n",
      "so closely  1\n",
      "not much  1\n",
      "so hard  1\n",
      "there no  1\n",
      "too far away  1\n",
      "extremely ugly  1\n",
      "really very  1\n",
      "i soon  1\n",
      "peacefully away  1\n",
      "very closely  1\n",
      "i not  1\n",
      "easily enough  1\n",
      "very quickly  1\n",
      "very close  1\n",
      "just as  1\n",
      "now completely  1\n",
      "not only  1\n",
      "Length of adverb phrase sentences:  110\n"
     ]
    }
   ],
   "source": [
    "# Now we look for \"adverb phrases\" or chunks that have 2 consecutive adverbs ('RB')\n",
    "# First step: writing a grammar that defines POS rules of the adverb phrase the chunk\n",
    "# we name this grammar \"ADVPH\" (\"ADVerb PHrase\")\n",
    "# Again, feel free to edit the regex in this grammar, or come up with your own!\n",
    "grammar_advph = \"ADVPH: {<RB>+<RB>}\"\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adv = nltk.RegexpParser(grammar_advph)\n",
    "\n",
    "advph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adv.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADVPH':\n",
    "                advph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adverb_phrases = []\n",
    "for sent in advph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adverb_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adverb phrases: ', adverb_phrases[:10])\n",
    "\n",
    "# top 50 adjective phrases\n",
    "freq_advph = nltk.FreqDist(adverb_phrases)\n",
    "\n",
    "print('Top adverb phrases by frequency: ')\n",
    "for word, freq in freq_advph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adverb phrase sentences: ', len(advph_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      freq\n",
      "adv                       \n",
      "so then                  5\n",
      "n't much                 1\n",
      "certainly very much      1\n",
      "now i                    1\n",
      "very slowly              2\n",
      "...                    ...\n",
      "never yet                1\n",
      "not entirely             1\n",
      "so now                   1\n",
      "surely not               1\n",
      "very carefully           1\n",
      "\n",
      "[80 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#freq_advph\n",
    "df_fadv = pd.DataFrame.from_dict(freq_advph, orient='index')\n",
    "df_fadv.columns = ['freq']\n",
    "df_fadv.index.name = 'adv'\n",
    "print(df_fadv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freqfreq2 = df_fadv['freq'].value_counts()\n",
    "\n",
    "freqfreq2.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little 255\n",
      "good 46\n",
      "more 25\n",
      "other 22\n",
      "old 19\n",
      "great 19\n",
      "small 17\n",
      "much 17\n",
      "first 16\n",
      "last 15\n",
      "true 14\n",
      "able 13\n",
      "beautiful 12\n",
      "important 12\n",
      "same 12\n",
      "long 10\n",
      "such 9\n",
      "sure 9\n",
      "sad 9\n",
      "bad 9\n",
      "single 9\n",
      "happy 9\n",
      "conceited 9\n",
      "only 8\n",
      "magnificent 7\n",
      "difficult 7\n",
      "enough 7\n",
      "possible 7\n",
      "extinct 7\n",
      "many 6\n",
      "unique 6\n",
      "fresh 6\n",
      "different 6\n",
      "responsible 6\n",
      "proud 5\n",
      "second 5\n",
      "new 5\n",
      "big 5\n",
      "invisible 5\n",
      "own 5\n",
      "golden 5\n",
      "present 5\n",
      "general 5\n",
      "afraid 5\n",
      "whole 4\n",
      "extraordinary 4\n",
      "funny 4\n",
      "asteroid 4\n",
      "serious 4\n",
      "turkish 4\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adjective tokens\n",
    "\n",
    "adjective_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['JJ', 'JJR', 'JJS']: # adjective, comparative, superlative (can you think of another way to write this regex?...)\n",
    "            if len(word)>1:\n",
    "                adjective_tokens.append(word)\n",
    "freq_adjective = nltk.FreqDist(adjective_tokens)\n",
    "\n",
    "for word, freq in freq_adjective.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not 166\n",
      "very 81\n",
      "so 78\n",
      "then 74\n",
      "too 43\n",
      "never 41\n",
      "again 36\n",
      "just 33\n",
      "now 31\n",
      "n't 29\n",
      "only 26\n",
      "here 24\n",
      "away 22\n",
      "once 21\n",
      "yes 19\n",
      "always 17\n",
      "also 17\n",
      "back 17\n",
      "much 16\n",
      "more 16\n",
      "even 16\n",
      "still 13\n",
      "there 12\n",
      "ever 11\n",
      "already 10\n",
      "down 10\n",
      "far 10\n",
      "well 10\n",
      "alone 9\n",
      "most 9\n",
      "certainly 9\n",
      "perhaps 9\n",
      "as 9\n",
      "yet 8\n",
      "first 7\n",
      "really 6\n",
      "ago 6\n",
      "slowly 6\n",
      "exactly 6\n",
      "long 6\n",
      "else 5\n",
      "about 5\n",
      "altogether 5\n",
      "instead 4\n",
      "scarcely 4\n",
      "carefully 4\n",
      "however 4\n",
      "somewhere 4\n",
      "no 4\n",
      "longer 4\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adverb tokens\n",
    "\n",
    "adverb_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['RB', 'RBR', 'RBS']: # adverb, comparative, superlative (again, you can look for ways to write this better!)\n",
    "            if len(word)>1:\n",
    "                adverb_tokens.append(word)\n",
    "freq_adverb = nltk.FreqDist(adverb_tokens)\n",
    "\n",
    "for word, freq in freq_adverb.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO / YOUR TURN NOW!\n",
    "## NOUN EXTRACTION\n",
    "## VERB EXTRACTION\n",
    "## REMEMBER TO CHECK THE PENN POS TAGS LIST: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "## TO FIND ALL TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prince 185\n",
      "planet 69\n",
      "flower 55\n",
      "time 45\n",
      "stars 41\n",
      "sheep 39\n",
      "fox 36\n",
      "man 35\n",
      "nothing 32\n",
      "day 31\n",
      "morning 31\n",
      "king 31\n",
      "geographer 23\n",
      "men 22\n",
      "anything 21\n",
      "life 21\n",
      "grown-ups 20\n",
      "earth 20\n",
      "drawing 18\n",
      "water 18\n",
      "ah 17\n",
      "night 16\n",
      "consequence 16\n",
      "baobabs 16\n",
      "desert 15\n",
      "friend 15\n",
      "one 15\n",
      "lamp 15\n",
      "people 14\n",
      "question 14\n",
      "thing 14\n",
      "flowers 14\n",
      "order 13\n",
      "orders 13\n",
      "snake 13\n",
      "years 12\n",
      "boa 12\n",
      "place 12\n",
      "heart 12\n",
      "thorns 12\n",
      "volcanoes 12\n",
      "lamplighter 12\n",
      "number 11\n",
      "something 11\n",
      "world 11\n",
      "matters 11\n",
      "everything 11\n",
      "way 11\n",
      "sunset 11\n",
      "use 11\n"
     ]
    }
   ],
   "source": [
    "# Top 50 noun tokens\n",
    "\n",
    "noun_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['NN', 'NNS', 'NNP', 'NNPS']: #noun, noun plural, proper noun sing, proper noun pl\n",
    "            if len(word)>1:\n",
    "                noun_tokens.append(word)\n",
    "freq_noun = nltk.FreqDist(noun_tokens)\n",
    "\n",
    "for word, freq in freq_noun.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 298\n",
      "said 195\n",
      "was 193\n",
      "have 143\n",
      "are 136\n",
      "be 106\n",
      "do 83\n",
      "had 79\n",
      "am 56\n",
      "has 38\n",
      "were 37\n",
      "made 37\n",
      "know 36\n",
      "did 35\n",
      "been 32\n",
      "come 32\n",
      "see 30\n",
      "make 29\n",
      "asked 27\n",
      "does 27\n",
      "say 23\n",
      "go 23\n",
      "look 23\n",
      "understand 22\n",
      "answered 20\n",
      "came 20\n",
      "replied 19\n",
      "want 18\n",
      "put 18\n",
      "added 17\n",
      "looked 16\n",
      "went 16\n",
      "saw 15\n",
      "took 14\n",
      "live 14\n",
      "think 14\n",
      "rose 14\n",
      "seemed 12\n",
      "looking 12\n",
      "set 11\n",
      "draw 11\n",
      "take 11\n",
      "tell 11\n",
      "love 11\n",
      "laughed 11\n",
      "let 11\n",
      "seen 10\n",
      "find 10\n",
      "demanded 10\n",
      "get 10\n"
     ]
    }
   ],
   "source": [
    "# Top 50 verb tokens\n",
    "\n",
    "verb_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']: #verb, past, present, past p, sing present, 3rd person sing\n",
    "            if len(word)>1:\n",
    "                verb_tokens.append(word)\n",
    "freq_verb = nltk.FreqDist(verb_tokens)\n",
    "\n",
    "for word, freq in freq_verb.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of adjective phrase sentences:  183\n",
      "Length of adverb phrase sentences:  110\n",
      "Length of adjectives:  1160\n",
      "Length of adverbs:  1230\n",
      "Length of nouns:  2949\n",
      "Length of verbs:  3381\n"
     ]
    }
   ],
   "source": [
    "print('Length of adjective phrase sentences: ', len(adjph_tags))\n",
    "print('Length of adverb phrase sentences: ', len(advph_tags))\n",
    "print('Length of adjectives: ', len(adjective_tokens))\n",
    "print('Length of adverbs: ', len(adverb_tokens))\n",
    "print('Length of nouns: ', len(noun_tokens))\n",
    "print('Length of verbs: ', len(verb_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of POS tags combinations we can compare\n",
    "# We need to get the sentences back from the tagging exercise and run some stats\n",
    "\n",
    "# Create a list of original sentences from the ADJECTIVE phrase subset:\n",
    "adjph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in adjph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        adjph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "# Create a list of original sentences from the ADVERB phrase subset:\n",
    "advph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in advph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        advph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(advph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL STEP: Combine lists together to have a single list of adjective/adverb phrases:\n",
    "# Useful to know which sentences are heavy in qualifiers\n",
    "\n",
    "# create a new variable to store all adjective phrase sentences\n",
    "adv_adj_phrase_sentences = adjph_whole_sentences\n",
    "\n",
    "# iterate over adverb phrase sentences\n",
    "for sent in advph_whole_sentences:\n",
    "    # if a sentence is not in the adjective phrases list imported\n",
    "    if sent not in adv_adj_phrase_sentences:\n",
    "        # attach that sentence\n",
    "        adv_adj_phrase_sentences.append(sent)\n",
    "\n",
    "# print the lenght of the list (i.e. number of sentences with both adjective and adverb phrases)\n",
    "print(len(adv_adj_phrase_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5465872156013\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, Writing Structural Programs chapter\n",
    "# section on Procedural vs Declarative style (http://www.nltk.org/book_1ed/ch04.html) \n",
    "\n",
    "## CORPUS STATISTICS--SENTENCES LENGTH\n",
    "\n",
    "# Calculating the average length of sentences in the entire corpus\n",
    "# from http://www.nltk.org/book_1ed/ch04.html\n",
    "total_corpus = sum(len(sent) for sent in textsplit) # remember: 'textsplit' is our text split into sentences\n",
    "print(total_corpus / len(textsplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.849137931034482\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average length of an adjective phrase sentence\n",
    "# We can then compare the average length of the adjective phrases to \n",
    "# the average sentences we calculated for all sentences in the corpus\n",
    "total_adjph_sentences = sum(len(sent) for sent in adjph_whole_sentences) # adjph_whole_sentences stores our adjective phrases\n",
    "print(total_adjph_sentences / len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
