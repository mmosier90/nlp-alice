{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbf5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5494da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carroll-alice.txt\n"
     ]
    }
   ],
   "source": [
    "file0 = nltk.corpus.gutenberg.fileids()[7]\n",
    "print(file0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a98d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alicetext = nltk.corpus.gutenberg.raw(file0)\n",
    "len(alicetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2038c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33493"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TOKENIZE\n",
    "alicetokens = nltk.word_tokenize(alicetext)\n",
    "len(alicetokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80adae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33493"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOWER\n",
    "alicewords = [w.lower() for w in alicetokens]\n",
    "len(alicewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d6ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2836"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete repeated words;  we won't use this but it's good to know how\n",
    "alicevocab = sorted(set(alicewords))\n",
    "len(alicevocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad93737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FREQUENCY\n",
    "from nltk import FreqDist\n",
    "fdist = FreqDist(alicewords)\n",
    "fdistkeys = list(fdist.keys())\n",
    "fdist['alice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6331aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 2418)\n",
      "('the', 1616)\n",
      "(\"'\", 1309)\n",
      "('.', 975)\n",
      "('and', 810)\n",
      "('to', 720)\n",
      "('a', 631)\n",
      "('she', 544)\n",
      "('it', 539)\n",
      "('i', 533)\n"
     ]
    }
   ],
   "source": [
    "topkeys = fdist.most_common(10)\n",
    "for pair in topkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac6b5750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 0.0721941898307109)\n",
      "('the', 0.0482488878273072)\n",
      "(\"'\", 0.0390827934195205)\n",
      "('.', 0.02911056041560923)\n",
      "('and', 0.0241841578837369)\n",
      "('to', 0.021497029229988356)\n",
      "('a', 0.01883975756128146)\n",
      "('she', 0.016242199862657868)\n",
      "('it', 0.016092914937449618)\n",
      "('i', 0.015913773027199714)\n"
     ]
    }
   ],
   "source": [
    "#normalized frequency\n",
    "numwords = len(alicewords)\n",
    "topkeysnorm=[(word, freq/numwords) for (word,freq) in topkeys]\n",
    "for pair in topkeysnorm:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e181f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", \t 2418\n",
      "the \t 1616\n",
      "' \t 1309\n",
      ". \t 975\n",
      "and \t 810\n",
      "to \t 720\n",
      "a \t 631\n",
      "she \t 544\n",
      "it \t 539\n",
      "i \t 533\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "ndist=FreqDist(alicewords)\n",
    "nitems = ndist.most_common(10)\n",
    "for item in nitems:\n",
    "    print (item[0], '\\t', item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef9a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "## NON-ALPHA FILTER\n",
    "pattern = re.compile('^[^a-z]+$')\n",
    "nonAlphaMatch = pattern.match('**')\n",
    "#  if it matched, print a message\n",
    "if nonAlphaMatch:\n",
    "    print ('matched non-alphabetical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7363ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdba3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', \"'s\", 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'chapter', 'i', 'down', 'the', 'rabbit-hole', 'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'alice', \"'without\", 'pictures', 'or', 'conversation', 'so', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', 'as', 'well', 'as', 'she', 'could', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', 'whether', 'the', 'pleasure', 'of', 'making', 'a']\n",
      "27155\n"
     ]
    }
   ],
   "source": [
    "# apply the function to emmawords\n",
    "alphaalicewords = [w for w in alicewords if not alpha_filter(w)]\n",
    "print(alphaalicewords[:100])\n",
    "print(len(alphaalicewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "410d93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STOPWORDS\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\"]\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f847ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12561\n"
     ]
    }
   ],
   "source": [
    "stoppedalicewords = [w for w in alphaalicewords if not w in stopwords]\n",
    "print(len(stoppedalicewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7436907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 462)\n",
      "('alice', 396)\n",
      "('little', 128)\n",
      "('one', 99)\n",
      "('know', 88)\n",
      "('like', 85)\n",
      "('went', 83)\n",
      "('queen', 75)\n",
      "('thought', 74)\n",
      "('time', 68)\n"
     ]
    }
   ],
   "source": [
    "## FREQUENCY #2\n",
    "alicedist = FreqDist(stoppedalicewords)\n",
    "aliceitems = alicedist.most_common(10)\n",
    "for item in aliceitems:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3177b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 0.03678051110580368)\n",
      "('alice', 0.03152615237640315)\n",
      "('little', 0.010190271475201018)\n",
      "('one', 0.007881538094100788)\n",
      "('know', 0.007005811639200701)\n",
      "('like', 0.006766977151500677)\n",
      "('went', 0.006607754159700661)\n",
      "('queen', 0.0059708621925005975)\n",
      "('thought', 0.0058912506966005895)\n",
      "('time', 0.005413581721200542)\n"
     ]
    }
   ],
   "source": [
    "#normalized frequency\n",
    "numwords2 = len(stoppedalicewords)\n",
    "topkeys2norm=[(word, freq/numwords2) for (word,freq) in aliceitems]\n",
    "for pair in topkeys2norm:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfa517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c517065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'alice'), ('alice', \"'s\"), (\"'s\", 'adventures'), ('adventures', 'in'), ('in', 'wonderland'), ('wonderland', 'by'), ('by', 'lewis'), ('lewis', 'carroll'), ('carroll', '1865'), ('1865', ']'), (']', 'chapter'), ('chapter', 'i'), ('i', '.'), ('.', 'down'), ('down', 'the'), ('the', 'rabbit-hole'), ('rabbit-hole', 'alice'), ('alice', 'was'), ('was', 'beginning'), ('beginning', 'to')]\n"
     ]
    }
   ],
   "source": [
    "## BIGRAMS\n",
    "# use \"words\", don't remove stopwords and symbols yet\n",
    "alicebigrams = list(nltk.bigrams(alicewords))\n",
    "print(alicebigrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b14fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69826f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(alicewords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "064635cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.013734213119159228)\n",
      "((',', \"'\"), 0.012808646582868063)\n",
      "((\"'\", 'said'), 0.009852805063744663)\n",
      "(('!', \"'\"), 0.00844952676678709)\n",
      "(('.', \"'\"), 0.007822530080912429)\n",
      "(('said', 'the'), 0.006180395903621652)\n",
      "((\"'\", 'i'), 0.005045830472038933)\n",
      "(('?', \"'\"), 0.004687546651539127)\n",
      "(('of', 'the'), 0.0038216940853312634)\n",
      "(('said', 'alice'), 0.003433553279789807)\n"
     ]
    }
   ],
   "source": [
    "for bscore in scored[:10]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01d0d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('said', 'the'), 0.006180395903621652)\n",
      "(('of', 'the'), 0.0038216940853312634)\n",
      "(('said', 'alice'), 0.003433553279789807)\n",
      "(('in', 'a'), 0.0028961275490400978)\n",
      "(('and', 'the'), 0.002358701818290389)\n",
      "(('in', 'the'), 0.0023288448332487387)\n",
      "(('it', 'was'), 0.002179559908040486)\n",
      "(('the', 'queen'), 0.002060131967873884)\n",
      "(('to', 'the'), 0.002060131967873884)\n",
      "(('as', 'she'), 0.0018212760875406801)\n",
      "(('the', 'king'), 0.0018212760875406801)\n",
      "(('at', 'the'), 0.0017914191024990297)\n",
      "(('she', 'had'), 0.0017914191024990297)\n",
      "(('a', 'little'), 0.001761562117457379)\n",
      "(('i', \"'m\"), 0.0016719911623324277)\n",
      "(('she', 'was'), 0.0016719911623324277)\n",
      "(('mock', 'turtle'), 0.0016421341772907773)\n",
      "(('and', 'she'), 0.0015824202072074762)\n",
      "(('the', 'mock'), 0.0015824202072074762)\n",
      "(('do', \"n't\"), 0.0015525632221658257)\n",
      "(('the', 'gryphon'), 0.0015525632221658257)\n",
      "(('the', 'hatter'), 0.0015525632221658257)\n",
      "(('to', 'be'), 0.0015227062371241753)\n",
      "(('went', 'on'), 0.0014331352819992238)\n",
      "(('to', 'herself'), 0.0013435643268742722)\n",
      "(('you', 'know'), 0.0012838503567909713)\n",
      "(('the', 'duchess'), 0.0011942794016660198)\n",
      "(('said', 'to'), 0.0011644224166243694)\n",
      "(('out', 'of'), 0.0011047084465410683)\n",
      "(('i', 'do'), 0.0010748514614994178)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36a8f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('said', 'alice'), 0.003433553279789807)\n",
      "(('mock', 'turtle'), 0.0016421341772907773)\n",
      "(('march', 'hare'), 0.0009255665362911653)\n",
      "(('thought', 'alice'), 0.0007762816110829129)\n",
      "(('white', 'rabbit'), 0.0006568536709163109)\n",
      "(('alice', 'thought'), 0.00035828382049980594)\n",
      "((\"'of\", 'course'), 0.00032842683545815545)\n",
      "(('alice', 'said'), 0.00032842683545815545)\n",
      "(('poor', 'alice'), 0.00032842683545815545)\n",
      "(('alice', 'replied'), 0.00026871286537485446)\n",
      "(('alice', 'looked'), 0.00023885588033320396)\n",
      "(('king', 'said'), 0.00023885588033320396)\n",
      "(('little', 'thing'), 0.00023885588033320396)\n",
      "(('poor', 'little'), 0.00023885588033320396)\n",
      "(('alice', 'began'), 0.00020899889529155347)\n",
      "(('cried', 'alice'), 0.00020899889529155347)\n",
      "(('good', 'deal'), 0.00020899889529155347)\n",
      "(('oh', 'dear'), 0.00020899889529155347)\n",
      "(('beautiful', 'soup'), 0.00017914191024990297)\n",
      "(('golden', 'key'), 0.00017914191024990297)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove stop words\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a5c345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((\"'any\", 'shrimp'), 15.03157198495453)\n",
      "((\"'cheshire\", 'puss'), 15.03157198495453)\n",
      "((\"'orange\", 'marmalade'), 15.03157198495453)\n",
      "((\"'ou\", 'est'), 15.03157198495453)\n",
      "((\"'rule\", 'forty-two'), 15.03157198495453)\n",
      "((\"'seven\", 'jogged'), 15.03157198495453)\n",
      "((\"'than\", 'waste'), 15.03157198495453)\n",
      "((\"'with\", 'extras'), 15.03157198495453)\n",
      "(('abide', 'figures'), 15.03157198495453)\n",
      "(('barking', 'hoarsely'), 15.03157198495453)\n",
      "(('bathing', 'machines'), 15.03157198495453)\n",
      "(('bright-eyed', 'terrier'), 15.03157198495453)\n",
      "(('buttered', 'toast'), 15.03157198495453)\n",
      "(('canvas', 'bag'), 15.03157198495453)\n",
      "(('carroll', '1865'), 15.03157198495453)\n",
      "(('crocodile', 'improve'), 15.03157198495453)\n",
      "(('daresay', \"it's\"), 15.03157198495453)\n",
      "(('deepest', 'contempt'), 15.03157198495453)\n",
      "(('draggled', 'feathers'), 15.03157198495453)\n",
      "(('edgar', 'atheling'), 15.03157198495453)\n"
     ]
    }
   ],
   "source": [
    "### PMI\n",
    "finder3 = BigramCollocationFinder.from_words(alicewords)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e89a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('play', 'croquet'), 11.768537579120737)\n",
      "(('golden', 'key'), 11.639254562175768)\n",
      "(('kid', 'gloves'), 11.572140366317234)\n",
      "((\"'of\", 'course'), 10.262184913095947)\n",
      "(('white', 'kid'), 10.124681389346012)\n",
      "(('beautiful', 'soup'), 9.987177865596077)\n",
      "(('three', 'gardeners'), 9.972678295900963)\n",
      "(('march', 'hare'), 9.94410914370419)\n",
      "(('good', 'deal'), 9.669001905569822)\n",
      "(('cheshire', 'cat'), 9.598612577678425)\n",
      "(('trembling', 'voice'), 9.183575078399581)\n",
      "(('mock', 'turtle'), 9.147595781294013)\n",
      "(('next', 'witness'), 9.124681389346012)\n",
      "(('feet', 'high'), 9.105572566398308)\n",
      "(('white', 'rabbit'), 9.029524156305673)\n",
      "(('great', 'hurry'), 8.871700648176141)\n",
      "(('right', 'size'), 8.746169766092283)\n",
      "(('offended', 'tone'), 8.709643890067168)\n",
      "(('oh', 'dear'), 8.572140366317232)\n",
      "(('another', 'moment'), 7.93987215081772)\n",
      "(('little', 'golden'), 7.54614515778429)\n",
      "(('came', 'upon'), 7.331132266813439)\n",
      "(('poor', 'little'), 6.3311322668134355)\n",
      "(('little', 'door'), 5.709643890067168)\n",
      "(('little', 'thing'), 5.416862140839321)\n",
      "(('poor', 'alice'), 5.161207265371127)\n",
      "(('thought', 'alice'), 4.893201717387063)\n",
      "(('cried', 'alice'), 4.887642192045162)\n",
      "(('alice', 'replied'), 4.714159371189659)\n",
      "(('said', 'alice'), 4.395956374403237)\n"
     ]
    }
   ],
   "source": [
    "# to get good results, must first apply frequency filter\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655977e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
