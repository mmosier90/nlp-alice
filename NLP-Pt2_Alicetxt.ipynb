{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carroll-alice.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import re \n",
    "\n",
    "file0 = nltk.corpus.gutenberg.fileids()[7]\n",
    "print(file0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alicetext = nltk.corpus.gutenberg.raw(file0)\n",
    "len(alicetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\", \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\", 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.', \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\", 'Oh dear!', \"I shall be late!'\", '(when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but when the Rabbit actually TOOK A WATCH\\nOUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\\nAlice started to her feet, for it flashed across her mind that she had\\nnever before seen a rabbit with either a waistcoat-pocket, or a watch\\nto take out of it, and burning with curiosity, she ran across the field\\nafter it, and fortunately was just in time to see it pop down a large\\nrabbit-hole under the hedge.', 'In another moment down went Alice after it, never once considering how\\nin the world she was to get out again.', 'The rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep\\nwell.', 'Either the well was very deep, or she fell very slowly, for she had\\nplenty of time as she went down to look about her and to wonder what was\\ngoing to happen next.']\n"
     ]
    }
   ],
   "source": [
    "textsplit = nltk.sent_tokenize(alicetext)\n",
    "print(textsplit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the word tokenizer to each sentence\n",
    "lowertext = [w.lower() for w in textsplit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "## NON-ALPHA FILTER\n",
    "pattern = re.compile('^[^a-z]+$')\n",
    "nonAlphaMatch = pattern.match('**')\n",
    "#  if it matched, print a message\n",
    "if nonAlphaMatch:\n",
    "    print ('matched non-alphabetical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[alice's adventures in wonderland by lewis carroll 1865]\\n\\nchapter i.\", \"down the rabbit-hole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought alice 'without pictures or\\nconversation?'\", 'so she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a white rabbit with pink eyes ran\\nclose by her.', \"there was nothing so very remarkable in that; nor did alice think it so\\nvery much out of the way to hear the rabbit say to itself, 'oh dear!\", 'oh dear!', \"i shall be late!'\", '(when she thought it over afterwards, it\\noccurred to her that she ought to have wondered at this, but at the time\\nit all seemed quite natural); but when the rabbit actually took a watch\\nout of its waistcoat-pocket, and looked at it, and then hurried on,\\nalice started to her feet, for it flashed across her mind that she had\\nnever before seen a rabbit with either a waistcoat-pocket, or a watch\\nto take out of it, and burning with curiosity, she ran across the field\\nafter it, and fortunately was just in time to see it pop down a large\\nrabbit-hole under the hedge.', 'in another moment down went alice after it, never once considering how\\nin the world she was to get out again.', 'the rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep\\nwell.', 'either the well was very deep, or she fell very slowly, for she had\\nplenty of time as she went down to look about her and to wonder what was\\ngoing to happen next.']\n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "# apply the function\n",
    "alphaalicewords = [w for w in lowertext if not alpha_filter(w)]\n",
    "print(alphaalicewords[:10])\n",
    "print(len(alphaalicewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the word tokenizer to each sentence\n",
    "tokentext = [nltk.word_tokenize(sent) for sent in alphaalicewords]\n",
    "#print(tokentext[:2])\n",
    "#the output is a list of strings that contains the sentences\n",
    "type(tokentext)\n",
    "#len(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('[', 'JJ'), ('alice', 'NN'), (\"'s\", 'POS'), ('adventures', 'NNS'), ('in', 'IN'), ('wonderland', 'NN'), ('by', 'IN'), ('lewis', 'NN'), ('carroll', 'NN'), ('1865', 'CD'), (']', 'NNP'), ('chapter', 'NN'), ('i', 'NN'), ('.', '.')], [('down', 'IN'), ('the', 'DT'), ('rabbit-hole', 'JJ'), ('alice', 'NN'), ('was', 'VBD'), ('beginning', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('very', 'RB'), ('tired', 'JJ'), ('of', 'IN'), ('sitting', 'VBG'), ('by', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), ('on', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('of', 'IN'), ('having', 'VBG'), ('nothing', 'NN'), ('to', 'TO'), ('do', 'VB'), (':', ':'), ('once', 'RB'), ('or', 'CC'), ('twice', 'VB'), ('she', 'PRP'), ('had', 'VBD'), ('peeped', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('book', 'NN'), ('her', 'PRP$'), ('sister', 'NN'), ('was', 'VBD'), ('reading', 'VBG'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('had', 'VBD'), ('no', 'DT'), ('pictures', 'NNS'), ('or', 'CC'), ('conversations', 'NNS'), ('in', 'IN'), ('it', 'PRP'), (',', ','), (\"'and\", 'VB'), ('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('a', 'DT'), ('book', 'NN'), (',', ','), (\"'\", \"''\"), ('thought', 'JJ'), ('alice', 'NN'), (\"'without\", 'POS'), ('pictures', 'NNS'), ('or', 'CC'), ('conversation', 'NN'), ('?', '.'), (\"'\", \"''\")]]\n"
     ]
    }
   ],
   "source": [
    "## POS Tagging, to retrieve adjective (JJs) and adverb (RBs) tags\n",
    "\n",
    "# use the Stanford POS tagger to POS tag tokens of each sentence\n",
    "# this is the default tagger in nltk\n",
    "taggedtext = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adjective phrases:  ['very tired ', 'very sleepy ', 'so very remarkable ', 'so very much ', 'quite natural ', 'very deep ', 'very deep ', 'too dark ', 'very likely ', 'very good ']\n",
      "Top adjective phrases by frequency: \n",
      "so much  8\n",
      "very curious  6\n",
      "very glad  5\n",
      "very much  4\n",
      "very little  4\n",
      "very likely  3\n",
      "very good  3\n",
      "so many  3\n",
      "only wish  3\n",
      "too much  3\n",
      "once more  3\n",
      "very tired  2\n",
      "quite natural  2\n",
      "very deep  2\n",
      "n't much  2\n",
      "very few  2\n",
      "as much  2\n",
      "so grave  2\n",
      "always ready  2\n",
      "very uncomfortable  2\n",
      "almost wish  2\n",
      "very short  2\n",
      "very difficult  2\n",
      "quite as much  2\n",
      "quite silent  2\n",
      "certainly too much  2\n",
      "not much  2\n",
      "so large  2\n",
      "n't very civil  2\n",
      "very interesting  2\n",
      "very sleepy  1\n",
      "so very remarkable  1\n",
      "so very much  1\n",
      "too dark  1\n",
      "rather glad  1\n",
      "no longer  1\n",
      "too large  1\n",
      "too small  1\n",
      "not much larger  1\n",
      "really impossible  1\n",
      "almost certain  1\n",
      "very nice  1\n",
      "now only ten  1\n",
      "too slippery  1\n",
      "very fond  1\n",
      "very small  1\n",
      "quite surprised  1\n",
      "quite dull  1\n",
      "now more  1\n",
      "so desperate  1\n",
      "Length of adjective phrase sentences:  236\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, chapter on Information Extraction--Chunking (https://www.nltk.org/book/ch07.html)\n",
    "\n",
    "# Using CHUNKING to parse sentences \n",
    "# to look for \"adjective phrases\", i.e. phrases (or chunks) that have adverbs and adjectives ('RB'+'JJ')\n",
    "# First step: writing a grammar that defines the POS in the chunk\n",
    "# we name this grammar \"ADJPH\" (\"ADJective PHrase\") using regexes \n",
    "\n",
    "import re\n",
    "grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\"\n",
    "# This regex reads as: \"find groups (\"< >\") of RBs (adverbs) together with groups of JJs (adjectives), with groups defineds as\n",
    "# RBs with any ending (the \".\" is a placeholder or wildcard for the \"R\" and the \"S\" at the end of RBR and RBS, \n",
    "# while \"?\" indicates \"optional character\" so RB can be found alone as well). Same regex operators apply to JJs.\n",
    "# Feel free to edit the regex in this grammar, or come up with your own.\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\n",
    "\n",
    "adjph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adj.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADJPH':\n",
    "                adjph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adjective_phrases = []\n",
    "for sent in adjph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adjective_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adjective phrases: ', adjective_phrases[:10])\n",
    "\n",
    "\n",
    "# Following our NLTK textbook, chapter 1 on Language Processing (https://www.nltk.org/book/ch01.html)\n",
    "\n",
    "## FREQUENCY DISTRIBUTIONS\n",
    "# Top 50 adjective phrases\n",
    "freq_adjph = nltk.FreqDist(adjective_phrases)\n",
    "\n",
    "print('Top adjective phrases by frequency: ')\n",
    "for word, freq in freq_adjph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adjective phrase sentences: ', len(adjph_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(adjective_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     freq\n",
      "adj                      \n",
      "very tired              2\n",
      "very sleepy             1\n",
      "so very remarkable      1\n",
      "so very much            1\n",
      "quite natural           2\n",
      "...                   ...\n",
      "near enough             1\n",
      "n't usual               1\n",
      "really clever           1\n",
      "most important          1\n",
      "alice dear              1\n",
      "\n",
      "[183 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#freq_adjph\n",
    "df_fadj = pd.DataFrame.from_dict(freq_adjph, orient='index')\n",
    "df_fadj.columns = ['freq']\n",
    "df_fadj.index.name = 'adj'\n",
    "print(df_fadj)\n",
    "\n",
    "#df = pd.DataFrame(freq_adjph, columns = ['word','freq'])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    153\n",
       "2     19\n",
       "3      6\n",
       "4      2\n",
       "8      1\n",
       "5      1\n",
       "6      1\n",
       "Name: freq, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fadj['freq'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "freqfreq = df_fadj['freq'].value_counts()\n",
    "\n",
    "freqfreq.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 adverb phrases:  ['as well ', 'so very ', 'so very ', 'never before ', 'never once ', 'suddenly down ', 'very slowly ', 'alice soon ', 'down here ', 'here alice ']\n",
      "Top adverb phrases by frequency: \n",
      "as well  15\n",
      "very soon  7\n",
      "as soon  6\n",
      "very politely  5\n",
      "so very  4\n",
      "down here  4\n",
      "very much  4\n",
      "just as well  4\n",
      "very well  3\n",
      "so far  3\n",
      "too far  3\n",
      "just then  3\n",
      "back again  3\n",
      "well enough  3\n",
      "down again  3\n",
      "very carefully  3\n",
      "so often  3\n",
      "not quite  3\n",
      "very nearly  3\n",
      "n't quite  3\n",
      "as long  3\n",
      "just now  3\n",
      "very slowly  2\n",
      "alice soon  2\n",
      "very earnestly  2\n",
      "not even  2\n",
      "so alice  2\n",
      "as hard  2\n",
      "rather not  2\n",
      "very gravely  2\n",
      "very humbly  2\n",
      "very angrily  2\n",
      "so easily  2\n",
      "quite as  2\n",
      "certainly too  2\n",
      "rather timidly  2\n",
      "'all right  2\n",
      "n't very  2\n",
      "'exactly so  2\n",
      "asleep again  2\n",
      "never even  2\n",
      "alice cautiously  2\n",
      "n't even  2\n",
      "never before  1\n",
      "never once  1\n",
      "suddenly down  1\n",
      "here alice  1\n",
      "not much  1\n",
      "not here before  1\n",
      "too long  1\n",
      "Length of adverb phrase sentences:  253\n"
     ]
    }
   ],
   "source": [
    "# Now we look for \"adverb phrases\" or chunks that have 2 consecutive adverbs ('RB')\n",
    "# First step: writing a grammar that defines POS rules of the adverb phrase the chunk\n",
    "# we name this grammar \"ADVPH\" (\"ADVerb PHrase\")\n",
    "# Again, feel free to edit the regex in this grammar, or come up with your own!\n",
    "grammar_advph = \"ADVPH: {<RB>+<RB>}\"\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adv = nltk.RegexpParser(grammar_advph)\n",
    "\n",
    "advph_tags = []\n",
    "for sent in taggedtext:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adv.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADVPH':\n",
    "                advph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "adverb_phrases = []\n",
    "for sent in advph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    adverb_phrases.append(temp)\n",
    "    \n",
    "print('First 10 adverb phrases: ', adverb_phrases[:10])\n",
    "\n",
    "# top 50 adjective phrases\n",
    "freq_advph = nltk.FreqDist(adverb_phrases)\n",
    "\n",
    "print('Top adverb phrases by frequency: ')\n",
    "for word, freq in freq_advph.most_common(50):\n",
    "    print(word, freq)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adverb phrase sentences: ', len(advph_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                freq\n",
      "adv                 \n",
      "as well           15\n",
      "so very            4\n",
      "never before       1\n",
      "never once         1\n",
      "suddenly down      1\n",
      "...              ...\n",
      "then again         1\n",
      "now hastily        1\n",
      "still just         1\n",
      "once again         1\n",
      "perhaps even       1\n",
      "\n",
      "[163 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#freq_advph\n",
    "df_fadv = pd.DataFrame.from_dict(freq_advph, orient='index')\n",
    "df_fadv.columns = ['freq']\n",
    "df_fadv.index.name = 'adv'\n",
    "print(df_fadv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freqfreq2 = df_fadv['freq'].value_counts()\n",
    "\n",
    "freqfreq2.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little 127\n",
      "other 40\n",
      "great 39\n",
      "much 34\n",
      "large 33\n",
      "more 33\n",
      "last 33\n",
      "first 32\n",
      "white 30\n",
      "such 27\n",
      "poor 26\n",
      "thought 24\n",
      "good 24\n",
      "long 24\n",
      "same 23\n",
      "next 20\n",
      "curious 19\n",
      "sure 19\n",
      "old 19\n",
      "right 18\n",
      "low 14\n",
      "high 14\n",
      "alice 14\n",
      "mad 14\n",
      "whole 13\n",
      "many 12\n",
      "best 11\n",
      "beautiful 11\n",
      "own 10\n",
      "glad 10\n",
      "small 10\n",
      "few 9\n",
      "different 9\n",
      "dear 9\n",
      "least 9\n",
      "afraid 8\n",
      "ready 8\n",
      "oh 7\n",
      "golden 7\n",
      "larger 7\n",
      "wish 7\n",
      "enough 7\n",
      "deep 6\n",
      "nice 6\n",
      "dry 6\n",
      "bright 6\n",
      "melancholy 6\n",
      "offended 6\n",
      "full 6\n",
      "sharp 6\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adjective tokens\n",
    "\n",
    "adjective_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['JJ', 'JJR', 'JJS']: # adjective, comparative, superlative (can you think of another way to write this regex?...)\n",
    "            if len(word)>1:\n",
    "                adjective_tokens.append(word)\n",
    "freq_adjective = nltk.FreqDist(adjective_tokens)\n",
    "\n",
    "for word, freq in freq_adjective.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n't 204\n",
      "very 139\n",
      "not 135\n",
      "so 110\n",
      "again 83\n",
      "then 82\n",
      "quite 52\n",
      "now 50\n",
      "alice 49\n",
      "just 49\n",
      "as 48\n",
      "here 48\n",
      "never 45\n",
      "only 42\n",
      "down 38\n",
      "once 32\n",
      "back 32\n",
      "well 30\n",
      "too 26\n",
      "up 26\n",
      "rather 25\n",
      "soon 25\n",
      "away 23\n",
      "yet 22\n",
      "ever 21\n",
      "even 19\n",
      "however 19\n",
      "first 18\n",
      "much 17\n",
      "indeed 16\n",
      "more 16\n",
      "there 15\n",
      "perhaps 14\n",
      "anxiously 14\n",
      "hastily 14\n",
      "suddenly 13\n",
      "still 13\n",
      "certainly 13\n",
      "far 13\n",
      "always 13\n",
      "hardly 12\n",
      "about 12\n",
      "else 11\n",
      "enough 11\n",
      "really 10\n",
      "nearly 10\n",
      "angrily 9\n",
      "together 9\n",
      "timidly 9\n",
      "close 8\n"
     ]
    }
   ],
   "source": [
    "# Top 50 adverb tokens\n",
    "\n",
    "adverb_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['RB', 'RBR', 'RBS']: # adverb, comparative, superlative (again, you can look for ways to write this better!)\n",
    "            if len(word)>1:\n",
    "                adverb_tokens.append(word)\n",
    "freq_adverb = nltk.FreqDist(adverb_tokens)\n",
    "\n",
    "for word, freq in freq_adverb.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO / YOUR TURN NOW!\n",
    "## NOUN EXTRACTION\n",
    "## VERB EXTRACTION\n",
    "## REMEMBER TO CHECK THE PENN POS TAGS LIST: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "## TO FIND ALL TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice 308\n",
      "queen 71\n",
      "time 68\n",
      "king 61\n",
      "hatter 56\n",
      "mock 56\n",
      "turtle 55\n",
      "gryphon 54\n",
      "way 53\n",
      "head 50\n",
      "thing 49\n",
      "voice 48\n",
      "rabbit 47\n",
      "duchess 42\n",
      "mouse 41\n",
      "tone 40\n",
      "dormouse 38\n",
      "'it 37\n",
      "cat 35\n",
      "march 34\n",
      "moment 31\n",
      "hare 31\n",
      "nothing 30\n",
      "things 30\n",
      "door 30\n",
      "eyes 28\n",
      "'you 27\n",
      "day 25\n",
      "course 25\n",
      "caterpillar 25\n",
      "round 23\n",
      "words 21\n",
      "minute 21\n",
      "sort 20\n",
      "feet 19\n",
      "anything 19\n",
      "hand 19\n",
      "dear 18\n",
      "house 18\n",
      "table 18\n",
      "something 18\n",
      "soup 18\n",
      "court 18\n",
      "use 17\n",
      "question 17\n",
      "side 17\n",
      "jury 17\n",
      "bill 17\n",
      "garden 16\n",
      "end 15\n"
     ]
    }
   ],
   "source": [
    "# Top 50 noun tokens\n",
    "\n",
    "noun_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['NN', 'NNS', 'NNP', 'NNPS']: #noun, noun plural, proper noun sing, proper noun pl\n",
    "            if len(word)>1:\n",
    "                noun_tokens.append(word)\n",
    "freq_noun = nltk.FreqDist(noun_tokens)\n",
    "\n",
    "for word, freq in freq_noun.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 462\n",
      "was 366\n",
      "had 184\n",
      "be 148\n",
      "do 124\n",
      "is 113\n",
      "'s 109\n",
      "know 88\n",
      "were 86\n",
      "have 84\n",
      "went 83\n",
      "did 74\n",
      "see 67\n",
      "began 58\n",
      "'m 56\n",
      "think 51\n",
      "say 51\n",
      "are 47\n",
      "go 47\n",
      "thought 46\n",
      "looked 45\n",
      "got 45\n",
      "get 44\n",
      "came 40\n",
      "'ve 39\n",
      "herself 38\n",
      "been 38\n",
      "'re 36\n",
      "made 30\n",
      "found 30\n",
      "put 30\n",
      "looking 30\n",
      "come 29\n",
      "tell 28\n",
      "replied 28\n",
      "seemed 27\n",
      "going 27\n",
      "make 27\n",
      "took 24\n",
      "heard 24\n",
      "alice 24\n",
      "felt 23\n",
      "added 23\n",
      "getting 22\n",
      "find 21\n",
      "does 21\n",
      "take 20\n",
      "like 20\n",
      "tried 19\n",
      "being 19\n"
     ]
    }
   ],
   "source": [
    "# Top 50 verb tokens\n",
    "\n",
    "verb_tokens = []\n",
    "for sentence in taggedtext:\n",
    "    for word, pos in sentence:\n",
    "        if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']: #verb, past, present, past p, sing present, 3rd person sing\n",
    "            if len(word)>1:\n",
    "                verb_tokens.append(word)\n",
    "freq_verb = nltk.FreqDist(verb_tokens)\n",
    "\n",
    "for word, freq in freq_verb.most_common(50):\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of adjective phrase sentences:  236\n",
      "Length of adverb phrase sentences:  253\n",
      "Length of adjectives:  1587\n",
      "Length of adverbs:  2213\n",
      "Length of nouns:  4992\n",
      "Length of verbs:  5673\n"
     ]
    }
   ],
   "source": [
    "print('Length of adjective phrase sentences: ', len(adjph_tags))\n",
    "print('Length of adverb phrase sentences: ', len(advph_tags))\n",
    "print('Length of adjectives: ', len(adjective_tokens))\n",
    "print('Length of adverbs: ', len(adverb_tokens))\n",
    "print('Length of nouns: ', len(noun_tokens))\n",
    "print('Length of verbs: ', len(verb_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of POS tags combinations we can compare\n",
    "# We need to get the sentences back from the tagging exercise and run some stats\n",
    "\n",
    "# Create a list of original sentences from the ADJECTIVE phrase subset:\n",
    "adjph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in adjph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        adjph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n"
     ]
    }
   ],
   "source": [
    "# Create a list of original sentences from the ADVERB phrase subset:\n",
    "advph_whole_sentences = []\n",
    "\n",
    "# loop over the sentences in the adjective phrase sentences we created:\n",
    "for sents in advph_tags:\n",
    "    temp=''\n",
    "    for (word,tag) in sents:\n",
    "        temp += word+' '\n",
    "        advph_whole_sentences.append(temp)\n",
    "        \n",
    "print(len(advph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL STEP: Combine lists together to have a single list of adjective/adverb phrases:\n",
    "# Useful to know which sentences are heavy in qualifiers\n",
    "\n",
    "# create a new variable to store all adjective phrase sentences\n",
    "adv_adj_phrase_sentences = adjph_whole_sentences\n",
    "\n",
    "# iterate over adverb phrase sentences\n",
    "for sent in advph_whole_sentences:\n",
    "    # if a sentence is not in the adjective phrases list imported\n",
    "    if sent not in adv_adj_phrase_sentences:\n",
    "        # attach that sentence\n",
    "        adv_adj_phrase_sentences.append(sent)\n",
    "\n",
    "# print the lenght of the list (i.e. number of sentences with both adjective and adverb phrases)\n",
    "print(len(adv_adj_phrase_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.26892307692307\n"
     ]
    }
   ],
   "source": [
    "# Following our NLTK textbook, Writing Structural Programs chapter\n",
    "# section on Procedural vs Declarative style (http://www.nltk.org/book_1ed/ch04.html) \n",
    "\n",
    "## CORPUS STATISTICS--SENTENCES LENGTH\n",
    "\n",
    "# Calculating the average length of sentences in the entire corpus\n",
    "# from http://www.nltk.org/book_1ed/ch04.html\n",
    "total_corpus = sum(len(sent) for sent in textsplit) # remember: 'textsplit' is our text split into sentences\n",
    "print(total_corpus / len(textsplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.30849478390462\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average length of an adjective phrase sentence\n",
    "# We can then compare the average length of the adjective phrases to \n",
    "# the average sentences we calculated for all sentences in the corpus\n",
    "total_adjph_sentences = sum(len(sent) for sent in adjph_whole_sentences) # adjph_whole_sentences stores our adjective phrases\n",
    "print(total_adjph_sentences / len(adjph_whole_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
